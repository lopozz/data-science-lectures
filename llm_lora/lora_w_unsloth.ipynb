{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29bb35a",
   "metadata": {
    "id": "f29bb35a"
   },
   "source": [
    "# Llama 3.2 LoRA fine-tuning with Unsloth\n",
    "\n",
    "Large Language Models (LLMs) used in Retrieval-Augmented Generation (RAG) pipelines often struggle when the retrieved context doesn‚Äôt contain the information needed to answer a user‚Äôs question.\n",
    "Smaller models, in particular, may hallucinate ‚Äî producing confident but incorrect answers instead of acknowledging missing evidence.\n",
    "\n",
    "In this notebook, we‚Äôll walk through fine-tuning Llama 3.2 using the Unsloth library to help the model:\n",
    "\n",
    "- Detect when the retrieved context is irrelevant or incomplete, and\n",
    "- Respond appropriately (e.g., ‚ÄúI couldn‚Äôt find that information in the provided materials.‚Äù)\n",
    "\n",
    "Beyond simply saying ‚ÄúI don‚Äôt know,‚Äù we‚Äôll also encourage the model to explain why it cannot provide a reliable answer.\n",
    "This approach makes the model‚Äôs behavior more transparent, trustworthy, and user-friendly ‚Äî reducing hallucinations while keeping responses informative and grounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc0484",
   "metadata": {},
   "source": [
    "# 0. The Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba9280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re\n",
    "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "#     !pip install unsloth\n",
    "# else:\n",
    "#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "#     import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "#     xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "#     !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "#     !pip install --no-deps unsloth\n",
    "# !pip install transformers==4.56.2\n",
    "# !pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e93566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/llm_lora/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/llm_lora/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.1: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA RTX 2000 Ada Generation Laptop GPU. Num GPUs = 1. Max memory: 7.653 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42bdf70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Quale era la principale caratteristica della struttura sociale dell'aristocrazia romana antica?\n",
      "\n",
      "context: Aristocrazia\n",
      "Laristocrazia (dal greco Œ¨œÅŒπœÉœÑŒøœÇ, √†ristos, \"migliore\" e Œ∫œÅŒ¨œÑŒøœÇ, kratos, \"comando\") √® una forma di governo nella quale poche persone (che secondo l'etimologia greca del termine dovrebbero essere i \"migliori\") controllano interamente lo Stato; secondo il pensiero platonico-aristotelico √® una delle tre forme di governo, assieme a monarchia e timocrazia, mentre l'oligarchia √® la sua forma degenerata; √® stata, assieme all'oligarchia, tra le forme di governo pi√π diffuse in Europa negli ultimi secoli, generalmente sotto forma di monarchie costituzionali, nelle quali il potere del sovrano √® controllato da un parlamento composto da soli nobili. Note Bibliografia Giuseppe Rensi, Forme di governo del passato e dell'avvenire, Roma, 1945 Arturo Beccari, Il pensiero politico classico, Milano, 1949 Lawrence Stone, The crisis of the aristocracy (1558-1641), Oxford, 1965 Alessandro Barbero, L'aristocrazia nella societ√† francese del Medioevo, Bologna, 1987. Voci correlate Democrazia Oligarchia Aristocrazia bizantina Aristocrazia nera Aristocrazia romana Nobilt√† Altri progetti Collegamenti esterni Forme di governo Forme di sovranit√†\n",
      "\n",
      "answer: Il testo fornito non fornisce alcuna informazione sulla struttura sociale dell'aristocrazia romana antica. Si concentra principalmente sulla definizione di aristocrazia, la sua relazione con altre forme di governo, e le sue caratteristiche generali, ma non descrive il funzionamento interno della societ√† romana. Non si possono dedurre informazioni sulla struttura sociale dell'aristocrazia romana senza ulteriori dati, come leggi, istituzioni, o la vita quotidiana della classe aristocratica.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "Sei un assistente utile e accurato. Rispondi solo utilizzando le informazioni presenti nel contesto fornito.\n",
    "Se il contesto non contiene dati sufficienti per rispondere, dillo chiaramente e spiega brevemente perch√©.\n",
    "Non fare supposizioni, attieniti alle informazioni nel contesto.\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"lopozz/UA4RAG-it\")\n",
    "\n",
    "\n",
    "context = ds[\"test\"][5][\"context\"]\n",
    "question = ds[\"test\"][5][\"question\"]\n",
    "\n",
    "\n",
    "user_prompt = f\"Contesto:\\n{context}\\n\\n{question}\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    ")\n",
    "\n",
    "[print(f\"{k}: {v}\\n\") for k, v in ds[\"test\"][5].items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bb79d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run: 1 - Secondo la definizione fornita, la principale caratteristica della struttura sociale dell'aristocrazia romana antica era il controllo esclusivo di poche persone di rara statura, ovvero i \"migliori\" (nelle parole greche).\n",
      "\n",
      "Tuttavia, non sono stati forniti dati specifici su come sia stata la struttura sociale dell'aristocrazia romana antica nella sua applicazione concreta nella pratica, come ad esempio la distribuzione del potere e delle ricchezze, il ruolo delle var\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 2 - La principale caratteristica della struttura sociale dell'aristocrazia romana antica era che era basata su una √©lite di poche famiglie riche e influenti, generalmente di nobili o di discendenza aristocratica, che detenevano il potere e la ricchezza. Questo tipo di societ√† era caratterizzata dalla concentrazione del potere e della ricchezza nelle mani di poche persone, a scapito delle masse della popolazione romana.\n",
      "\n",
      "Nel contesto dell'Aristocrazia √® menzion\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 3 - La principale caratteristica della struttura sociale dell'aristocrazia romana antica √® stata l'accumulo di potere e di ricchezza da parte di una piccola √©lite di famiglie e individui di nobile status, che detenevano il controllo economico e politico del governo e della societ√†.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 4 - La principale caratteristica della struttura sociale dell'aristocrazia romana antica era la propriet√† terriale, ovvero l'accumulo di ricchezza e potere basato sull' possesso di grandi quantit√† di terreno. Inoltre, la nobilt√† romana era composta da famiglie con una lunga tradizione di ricchezza, potere e influenza politica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 5 - La principale caratteristica della struttura sociale dell'aristocrazia romana antica era la loro ricchezza e potere economico.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 6 - Secondo le informazioni fornite, la principale caratteristica della struttura sociale dell'aristocrazia romana antica sarebbe stata \"un governo\" o \"solo\" una di \"un governo\" di poche persone che \"dovrebbero essere i'migliori'\" di una societ√†.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 7 - La principale caratteristica della struttura sociale dell'aristocrazia romana antica era il possesso di vasti poteri e ricchezze, ovvero il dominio economico e politico su una maggioranza di popolo.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 8 - La principale caratteristica della struttura sociale dell'aristocrazia romana antica era la posizione di potere e influenza esercitata da pochi individui molto ricchi e di nobilesse.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 9 - La principale caratteristica della struttura sociale dell'aristocrazia romana antica era il possesso di vasti benefici (latine \"munificendi\") che derivavano da un'attivit√† imprenditoriale particolarmente produttiva.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 10 - Secondo il contesto, una delle risposte fornite √® dell'antiquista Alessandro Barbero e cita \"L'aristocrazia nella societ√† francese del Medioevo\". Tuttavia, √® utile citare una risposta in pi√π. Lo storico Giuseppe Rensi, nel libro \"Forme di governo del passato e dell'avvenire\" scrive che, secondo il pensiero platonico-aristotelico, nella struttura sociale dell'aristocrazia romana, il potere era concentrato in poche mani\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "\n",
    "for i in range(num_samples):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=125,\n",
    "        temperature=1.0,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "        streamer=None,  # disable live streaming for capturing\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n",
    "    )\n",
    "    print(f\"\\nRun: {i + 1} - {generated_text}\")\n",
    "    print(\"----\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ddd78",
   "metadata": {
    "id": "7b7ddd78"
   },
   "source": [
    "## 1. Load and Prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c13886e",
   "metadata": {
    "id": "6c13886e"
   },
   "source": [
    "| role     | purpose                                                                                     |\n",
    "| -------- | ------------------------------------------------------------------------------------------- |\n",
    "| question | The user‚Äôs query to be answered.                             |\n",
    "| context  | Retrieved passages/snippets provided to the model (may be empty or irrelevant).   |\n",
    "| answer   | The model‚Äôs grounded output: a clear ‚Äúnot found‚Äù. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dcb860",
   "metadata": {},
   "source": [
    "We now use the `Llama3.2` format for conversation style finetunes. Llama3.2 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "I'm great thanks!<|eot_id|>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d05514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3443aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 24 Nov 2025\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Quali sono i criteri per l'identificazione di una sottoclasse all'interno della classe \"Aves\"?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Non √® possibile rispondere alla domanda in base al testo fornito.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Ok grazie!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Quali sono i criteri per l'identificazione di una sottoclasse all'interno della classe \\\"Aves\\\"?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Non √® possibile rispondere alla domanda in base al testo fornito.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Ok grazie!\"},\n",
    "]\n",
    "\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(tokenized_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cfa8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Sei un assistente utile e accurato. Rispondi solo utilizzando le informazioni presenti nel contesto fornito.\n",
    "Se il contesto non contiene dati sufficienti per rispondere, dillo chiaramente e spiega brevemente perch√©.\n",
    "Non fare supposizioni, attieniti alle informazioni nel contesto.\n",
    "\"\"\"\n",
    "\n",
    "ds = ds.map(\n",
    "    lambda ex: {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Contesto:\\n{ex['context'].replace('Eventi, invenzioni e scoperte ', '')}\\n\\n{ex['question']}\",\n",
    "            },\n",
    "            {\"role\": \"assistant\", \"content\": ex[\"answer\"]},\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35d302a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '\\nSei un assistente utile e accurato. Rispondi solo utilizzando le informazioni presenti nel contesto fornito.\\nSe il contesto non contiene dati sufficienti per rispondere, dillo chiaramente e spiega brevemente perch√©.\\nNon fare supposizioni, attieniti alle informazioni nel contesto.\\n',\n",
       "  'role': 'system'},\n",
       " {'content': 'Contesto:\\nAltri progetti 05\\n\\nQuale paese ha sviluppato il primo sistema di comunicazione satellitare?',\n",
       "  'role': 'user'},\n",
       " {'content': \"Non √® possibile rispondere alla domanda, anche solo con il testo fornito. Il testo non fornisce alcuna informazione specifica su quale paese abbia sviluppato il primo sistema di comunicazione satellitare. La domanda presuppone la conoscenza di un evento storico e di un progetto, ma il contesto non offre alcun dettaglio sull'origine del primo sistema.\\n\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][\"messages\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "311f0659",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(\n",
    "    lambda ex: {\n",
    "        \"text\": tokenizer.apply_chat_template(\n",
    "            ex[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4699d7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 18 Nov 2025\n",
      "\n",
      "Sei un assistente utile e accurato. Rispondi solo utilizzando le informazioni presenti nel contesto fornito.\n",
      "Se il contesto non contiene dati sufficienti per rispondere, dillo chiaramente e spiega brevemente perch√©.\n",
      "Non fare supposizioni, attieniti alle informazioni nel contesto.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Contesto:\n",
      "Bish≈çnen\n",
      "indica un canone estetico di bellezza maschile tipicamente giapponese. Il prefisso si riferisce specificamente alla bellezza femminile, a una bella donna. Il bish≈çnen pu√≤ essere inteso in pi√π di un modo: ragazzo magro e non molto muscoloso, con un mento affusolato e un'apparenza effeminata o androgina. Incarna l'ideale del giovane amante omosessuale; alcuni appassionati occidentali utilizzano questo termine per riferirsi a qualsiasi bel personaggio maschile, nonostante quest'uso (letteralmente parlando) sia impreciso. Viene ritratto negli anime e nei manga, specialmente in sh≈çjo, sh≈çnen'ai e yaoi. Il termine equivalente per indicare una ragazza molto bella √® bish≈çjo. Storia Il Giappone ha una tradizione di dongiovanni piuttosto femminei, il gusto per il travestitismo e l'ambiguit√†. Bench√© negli anni novanta questo fenomeno sociale si potesse notare nelle strade di Shibuya, nasce dal teatro kabuki e takarazuka, dove gli onnagata sono gli attori maschi che interpretano ruoli femminili. Non cercano d'interpretare una donna, ma concepiscono e portano in scena versioni idealizzate della donna. Gli onnagata, allevati fin da piccoli a raggiungere il massimo livello di femminilit√† e grazia, ben presto vanno conquistando popolarit√†. Nel teatro takarazuka sono frequenti le ambientazioni esotiche, i costumi sgargianti, l'ambiguit√†, le trame romantiche e melodrammatiche. Le otokoyaku sono le attrici che interpretano personaggi maschili: nel teatro takarazuka, poich√© per la prima volta possono interpretare ruoli che normalmente sono loro negati, immaginano nuove versioni della mascolinit√†. Queste nuove figure maschili, rispetto a quelle esistenti, sono molto pi√π dolci e romantiche. Da allora, l'immagine di questi nuovi modelli maschili, divenuti molto pi√π\n",
      "\n",
      "Quale materiale di costruzione era utilizzato per creare i primi modelli di legno per le prime macchine da stampa?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Il testo fornito non fornisce alcuna informazione sulla tecnologia utilizzata per la creazione dei primi modelli di legno per le prime macchine da stampa. Il testo si concentra invece sulla definizione del termine \"bish≈çnen\" e sulla storia della bellezza maschile e del travestimento in Giappone.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# We now have to apply the chat template for Gemma3 onto the conversations, and save it to text.\n",
    "print(ds[\"train\"][\"text\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d237f24",
   "metadata": {
    "id": "9d237f24"
   },
   "source": [
    "## 2. Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17edf2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "      (2-27): 26 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b6cc8cf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411,
     "referenced_widgets": [
      "bec979622f35461b88b6218ecff08d1d",
      "50a368dd3f8c48c889d6aa271a4560b4",
      "48bdc6ee888f45e699c7ae22c6c7973e",
      "18cdf2f4cda14beab466e66d9b5637c0",
      "8444c8b2148b4348b483db0d9a6833ce",
      "3e132807d1fe49be93ededc1c38504a6",
      "eebb74faae0f46d1b78c761a02b0b484",
      "6d1b5b8629fb4ea98e2ad72387fb76b7",
      "ccae9bc300694095ab5eacf559021269",
      "fa326aee2e3748a583e951cc97ed14f3",
      "32ac4c0474b641edae3e0be80dcdf795"
     ]
    },
    "id": "4b6cc8cf",
    "outputId": "ac06e33d-aab2-4af0-d4d7-acf39952936b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a840a7-2b67-4ed1-a36c-084a8af29039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 24 21:30:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 2000 Ada Gene...    Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   51C    P3             17W /   55W |    3177MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2814      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A    404201      C   ...-lectures/llm_lora/.venv/bin/python       3154MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f48c94",
   "metadata": {
    "id": "64f48c94"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Now let's train our model. We do 100 steps to speed things up, but you\n",
    "# can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=None,  # Can set up evaluation!\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,  # Use GA to mimic batch size!\n",
    "        warmup_steps=5,\n",
    "        num_train_epochs=1,  # Set this for 1 full training run.\n",
    "        # max_steps = 100,\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=None,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86766b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs.\n",
    "# This helps increase accuracy of finetunes!\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e585e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX 2000 Ada Generation Laptop GPU. Max memory = 7.653 GB.\n",
      "3.051 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c758238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 80 | Num Epochs = 1 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 1 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 194,510,848 of 3,407,260,672 (5.71% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.984200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.961600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.759500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.000100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08885236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.0898 seconds used for training.\n",
      "1.43 minutes used for training.\n",
      "Peak reserved memory = 7.271 GB.\n",
      "Peak reserved memory for training = 4.22 GB.\n",
      "Peak reserved memory % of max memory = 95.008 %.\n",
      "Peak reserved memory for training % of max memory = 55.142 %.\n"
     ]
    }
   ],
   "source": [
    "# Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a8685",
   "metadata": {
    "id": "068a8685"
   },
   "outputs": [],
   "source": [
    "# Save model locally\n",
    "\n",
    "# model_path = 'path/to/model'\n",
    "# model.save_pretrained(model_path)  # Local saving\n",
    "# tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7672c58b",
   "metadata": {
    "id": "7672c58b"
   },
   "source": [
    "## 5&nbsp;&nbsp;Inference\n",
    "**YOU MAY NEED TO RESTART THE KERNEL HERE TO CLEAR THE GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b242239b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/llm_lora/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/llm_lora/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.1: Fast Llama patching. Transformers: 4.57.1.\n",
      "   \\\\   /|    NVIDIA RTX 2000 Ada Generation Laptop GPU. Num GPUs = 1. Max memory: 7.653 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# load the LoRA adapters we just saved\n",
    "\n",
    "# from unsloth import FastLanguageModel\n",
    "# \n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=model_path,  # YOUR MODEL YOU USED FOR TRAINING\n",
    "#     max_seq_length=2048,\n",
    "#     load_in_4bit=True,\n",
    "# )\n",
    "\n",
    "from unsloth import FastModel\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    ")\n",
    "\n",
    "model.load_adapter(\"lopozz/Llama-3.2-3B-Instruct-UA4RAG-it-adapters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de008e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quale era la principale caratteristica della struttura sociale dell'aristocrazia romana antica?\n",
      "Aristocrazia\n",
      "Laristocrazia (dal greco Œ¨œÅŒπœÉœÑŒøœÇ, √†ristos, \"migliore\" e Œ∫œÅŒ¨œÑŒøœÇ, kratos, \"comando\") √® una forma di governo nella quale poche persone (che secondo l'etimologia greca del termine dovrebbero essere i \"migliori\") controllano interamente lo Stato; secondo il pensiero platonico-aristotelico √® una delle tre forme di governo, assieme a monarchia e timocrazia, mentre l'oligarchia √® la sua forma degenerata; √® stata, assieme all'oligarchia, tra le forme di governo pi√π diffuse in Europa negli ultimi secoli, generalmente sotto forma di monarchie costituzionali, nelle quali il potere del sovrano √® controllato da un parlamento composto da soli nobili. Note Bibliografia Giuseppe Rensi, Forme di governo del passato e dell'avvenire, Roma, 1945 Arturo Beccari, Il pensiero politico classico, Milano, 1949 Lawrence Stone, The crisis of the aristocracy (1558-1641), Oxford, 1965 Alessandro Barbero, L'aristocrazia nella societ√† francese del Medioevo, Bologna, 1987. Voci correlate Democrazia Oligarchia Aristocrazia bizantina Aristocrazia nera Aristocrazia romana Nobilt√† Altri progetti Collegamenti esterni Forme di governo Forme di sovranit√†\n",
      "\n",
      "Run: 1 - Il testo fornito non fornisce informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche generali, ma non specifica la struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 2 - Il testo fornito non contiene informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche generali, ma non fornisce informazioni specifiche sulla struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 3 - Il testo fornito non contiene informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche generali, ma non fornisce informazioni specifiche sulla struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 4 - Il testo fornito non contiene informazioni sulla struttura sociale dell'aristocrazia romana antica. La risposta non pu√≤ essere dedotta dal testo fornito.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 5 - Il testo fornito non contiene informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche generali, ma non fornisce informazioni specifiche sulla struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 6 - Il testo fornito non fornisce informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche, ma non fornisce alcuna informazione sulla struttura sociale specifica dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 7 - Il testo fornito non fornisce informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive la definizione e le caratteristiche dell'aristocrazia in generale, ma non specifica alcuna informazione sulla struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 8 - Il testo fornito non contiene informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche, ma non fornisce alcuna informazione sulla struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 9 - Il testo fornito non contiene informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche generali, ma non fornisce informazioni specifiche sulla struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Run: 10 - Il testo fornito non fornisce informazioni sulla struttura sociale dell'aristocrazia romana antica. Il testo descrive il concetto di aristocrazia e le sue caratteristiche generali, ma non specifica alcuna informazione sulla struttura sociale dell'aristocrazia romana antica.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Sei un assistente utile e accurato. Rispondi solo utilizzando le informazioni presenti nel contesto fornito.\n",
    "Se il contesto non contiene dati sufficienti per rispondere, dillo chiaramente e spiega brevemente perch√©.\n",
    "Non fare supposizioni, attieniti alle informazioni nel contesto.\n",
    "\"\"\"\n",
    "\n",
    "ds = load_dataset(\"lopozz/UA4RAG-it\")\n",
    "context = ds[\"test\"][5][\"context\"]\n",
    "question = ds[\"test\"][5][\"question\"]\n",
    "\n",
    "print(question)\n",
    "print(context)\n",
    "\n",
    "user_prompt = f\"Contesto:\\n{context}\\n\\n{question}\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "for i in range(num_samples):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=125,\n",
    "        temperature=0.3,\n",
    "        streamer=None,\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n",
    "    )\n",
    "    print(f\"\\nRun: {i + 1} - {generated_text}\")\n",
    "    print(\"----\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae94172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosa √® un organo a pompa?\n",
      "\n",
      "Run: 1 - Un organo a pompa √® un tipo di organo a serbatoio d'aria che consiste in una (o pi√π) tastiera, manuale, e due pedali per azionare i mantici per l'aria.\n",
      "--------------------------------------------------------------------------------\n",
      "In quale famiglia di aerofoni ad ancia libera rientra l'organo a pompa?\n",
      "\n",
      "Run: 2 - L'organo a pompa rientra nella famiglia di aerofoni ad ancia libera \"serie di aerofoni ad ancia libera\" (codice 412.132) secondo la classificazione Hornbostel-Sachs.\n",
      "--------------------------------------------------------------------------------\n",
      "Ci sono due tipi di organi a pompa: l'organo ad aria compressa e l'organo ad aria aspirata. Quali sono i nomi comuni di questi due tipi di organi?\n",
      "\n",
      "Run: 3 - I nomi comuni per questi due tipi di organi a pompa sono: \n",
      "\n",
      "- Organo ad aria compressa: armonium o armonio\n",
      "- Organo ad aria aspirata: reed organ\n",
      "--------------------------------------------------------------------------------\n",
      "Cosa significa antropologia?\n",
      "\n",
      "Run: 4 - L'antropologia √® una branca della scienza che studia l'uomo sotto diverse prospettive, come la societ√†, la cultura, la morfologia, la psicoevoluzione, la sociologia, l'arte, la filosofia e la religione.\n",
      "--------------------------------------------------------------------------------\n",
      "Da quale disciplina √® nata l'antropologia?\n",
      "\n",
      "Run: 5 - L'antropologia √® nata come disciplina interna alla biologia.\n",
      "--------------------------------------------------------------------------------\n",
      "Quali sono le diverse prospettive con cui l'antropologia studia l'essere umano?\n",
      "\n",
      "Run: 6 - Secondo il testo fornito, l'antropologia studia l'essere umano sotto diverse prospettive, tra cui:\n",
      "\n",
      "1. Sociale\n",
      "2. Culturale\n",
      "3. Morfologica\n",
      "4. Psicoevolutiva\n",
      "5. Sociologica\n",
      "6. Artistico-espressiva\n",
      "7. Filosofico-religiosa\n",
      "--------------------------------------------------------------------------------\n",
      "Cosa √® l'agricoltura?\n",
      "\n",
      "Run: 7 - La definizione fornita nel testo √®: \n",
      "\n",
      "L'agricoltura √® l'attivit√† umana che consiste nella coltivazione di specie vegetali.\n",
      "--------------------------------------------------------------------------------\n",
      "Qual √® lo scopo basilare dell'agricoltura?\n",
      "\n",
      "Run: 8 - Secondo il testo fornito, lo scopo basilare dell'agricoltura √® ottenere prodotti dalle piante, da utilizzare soprattutto a scopo alimentare.\n",
      "--------------------------------------------------------------------------------\n",
      "In economia, l'agricoltura rientra in quale settore?\n",
      "\n",
      "Run: 9 - L'agricoltura rientra nel settore primario.\n",
      "--------------------------------------------------------------------------------\n",
      "Cosa √® l'architettura?\n",
      "\n",
      "Run: 10 - L'architettura √® la disciplina che si occupa della progettazione e costruzione di immobili e ambienti costruiti, che concorrono aspetti tecnici ed artistici.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check for catastrophic forgetting!\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"ReDiX/wikipediaQA-ita\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Sei un assistente utile e accurato. Rispondi solo utilizzando le informazioni presenti nel contesto fornito.\n",
    "Se il contesto non contiene dati sufficienti per rispondere, dillo chiaramente e spiega brevemente perch√©.\n",
    "Non fare supposizioni, attieniti alle informazioni nel contesto.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "num_samples = 10\n",
    "\n",
    "for i in range(num_samples):\n",
    "    context = ds[\"train\"][i][\"context\"]\n",
    "    question = ds[\"train\"][i][\"question\"]\n",
    "\n",
    "    user_prompt = f\"Contesto:\\n{context}\\n\\n{question}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=125,\n",
    "        temperature=0.3,\n",
    "        streamer=None,\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n",
    "    )\n",
    "    print(question)\n",
    "    print(f\"\\nRun: {i + 1} - {generated_text}\")\n",
    "    print(\"----\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe80316",
   "metadata": {},
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e5780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4091ca198c047969007b8c9cf9d1d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ef86bc149040c9ae0e213574304e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to https://huggingface.co/lopozz/Llama-3.2-3B-Instruct-UA4RAG-it-adapters\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da6e22da51d40b0ba1c98bb6d998eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd89a679d52d45febb080bb0ab09b896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "token = os.environ[\"HUGGINGFACE_HUB_TOKEN\"]\n",
    "model_path = 'path/to/model'\n",
    "# Merge to 16bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\n",
    "        \"model_path\", tokenizer, save_method=\"merged_16bit\"\n",
    "    )\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\n",
    "        \"hf_user/model_path\", tokenizer, save_method=\"merged_16bit\", token=\"\"\n",
    "    )\n",
    "\n",
    "# Merge to 4bit\n",
    "if False:\n",
    "    model.save_pretrained_merged(\n",
    "        \"model_path\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_4bit\",\n",
    "    )\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub_merged(\n",
    "        \"hf_user/model_path\", tokenizer, save_method=\"merged_4bit\", token=\"\"\n",
    "    )\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model_path\")\n",
    "    tokenizer.save_pretrained(\"model_path\")\n",
    "if False:  # Pushing to HF Hub\n",
    "    model.push_to_hub(\"hf_user/model_path\", token=token)\n",
    "    tokenizer.push_to_hub(\n",
    "        \"hf_user/model_path\", token=token\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lora-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18cdf2f4cda14beab466e66d9b5637c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa326aee2e3748a583e951cc97ed14f3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_32ac4c0474b641edae3e0be80dcdf795",
      "value": "‚Äá1/3‚Äá[00:17&lt;00:21,‚Äá10.92s/it]"
     }
    },
    "32ac4c0474b641edae3e0be80dcdf795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3e132807d1fe49be93ededc1c38504a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48bdc6ee888f45e699c7ae22c6c7973e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6d1b5b8629fb4ea98e2ad72387fb76b7",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ccae9bc300694095ab5eacf559021269",
      "value": 1
     }
    },
    "50a368dd3f8c48c889d6aa271a4560b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e132807d1fe49be93ededc1c38504a6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_eebb74faae0f46d1b78c761a02b0b484",
      "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá‚Äá33%"
     }
    },
    "6d1b5b8629fb4ea98e2ad72387fb76b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8444c8b2148b4348b483db0d9a6833ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bec979622f35461b88b6218ecff08d1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50a368dd3f8c48c889d6aa271a4560b4",
       "IPY_MODEL_48bdc6ee888f45e699c7ae22c6c7973e",
       "IPY_MODEL_18cdf2f4cda14beab466e66d9b5637c0"
      ],
      "layout": "IPY_MODEL_8444c8b2148b4348b483db0d9a6833ce"
     }
    },
    "ccae9bc300694095ab5eacf559021269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eebb74faae0f46d1b78c761a02b0b484": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fa326aee2e3748a583e951cc97ed14f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
