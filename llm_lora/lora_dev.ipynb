{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1abb76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/lora_finetune/.venv/lib/python3.11/site-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def print_trainable_summary(model: torch.nn.Module) -> None:\n",
    "    total = 0\n",
    "    trainable = 0\n",
    "    trainable_names = []\n",
    "    for name, p in model.named_parameters():\n",
    "        n = p.numel()\n",
    "        total += n\n",
    "        if p.requires_grad:\n",
    "            trainable += n\n",
    "            trainable_names.append((name, n))\n",
    "    print(\n",
    "        f\"Total params: {total:,}; Trainable params: {trainable:,} ({100 * trainable / total:.4f}%)\"\n",
    "    )\n",
    "    print(\"Trainable parameter groups (name, numel):\")\n",
    "    for nm, nmels in trainable_names:\n",
    "        print(\" \", nm, nmels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50935ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Total params: 122,976,001; Trainable params: 122,976,001 (100.0000%)\n",
      "Trainable parameter groups (name, numel):\n",
      "  layer1.weight 64000\n",
      "  layer1.bias 6400\n",
      "  layer2.weight 40960000\n",
      "  layer2.bias 6400\n",
      "  layer3.weight 40960000\n",
      "  layer3.bias 6400\n",
      "  layer4.weight 40960000\n",
      "  layer4.bias 6400\n",
      "  output.weight 6400\n",
      "  output.bias 1\n"
     ]
    }
   ],
   "source": [
    "class BigMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BigMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "input_dim=10\n",
    "hidden_dim = 6400\n",
    "output_dim = 1\n",
    "model = BigMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "print_trainable_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9927b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/lora_finetune/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/lpozzi/Git/data-science-lectures/lora_finetune/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 53081.7775, Time: 3.13s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n",
      "Epoch [2/10], Loss: 331.7734, Time: 4.62s, GPU Memory: 1892.72 MB, GPU Utilization: 97%\n",
      "Epoch [3/10], Loss: 339.1843, Time: 6.10s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n",
      "Epoch [4/10], Loss: 340.9896, Time: 7.58s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n",
      "Epoch [5/10], Loss: 335.1193, Time: 9.06s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n",
      "Epoch [6/10], Loss: 333.3865, Time: 10.54s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n",
      "Epoch [7/10], Loss: 337.8707, Time: 12.01s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n",
      "Epoch [8/10], Loss: 332.7873, Time: 13.49s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n",
      "Epoch [9/10], Loss: 351.3964, Time: 14.97s, GPU Memory: 1892.72 MB, GPU Utilization: 97%\n",
      "Epoch [10/10], Loss: 335.5929, Time: 16.45s, GPU Memory: 1892.72 MB, GPU Utilization: 98%\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "        gpu_utilization = torch.cuda.utilization()  # percentage\n",
    "        return gpu_memory, gpu_utilization\n",
    "    return 0, 0\n",
    "\n",
    "num_samples=1000\n",
    "X = torch.randn(num_samples, input_dim)\n",
    "y = torch.sum(X, dim=1) + torch.randn(num_samples) * 0.1  # sum of inputs with noise\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "total_loss = 0\n",
    "epochs=10\n",
    "\n",
    "# Convert dataset to DataLoader for batch processing\n",
    "dataset = TensorDataset(X, y)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss += epoch_loss\n",
    "    elapsed_time = time.time() - start_time\n",
    "    gpu_memory, gpu_utilization = get_gpu_utilization()\n",
    "\n",
    "    # Print performance metrics every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, \"\n",
    "            f\"Time: {elapsed_time:.2f}s, \"\n",
    "            f\"GPU Memory: {gpu_memory:.2f} MB, \"\n",
    "            f\"GPU Utilization: {gpu_utilization}%\")\n",
    "\n",
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6146824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LoRAAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps an existing nn.Linear (or nn.Embedding) and adds low-rank adapters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, module: nn.Module, r: int = 4, alpha: int = None, init_scale: float = 1e-3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.module = module  # e.g., nn.Linear\n",
    "        self.r = r\n",
    "        if alpha is None:\n",
    "            alpha = r\n",
    "        self.scaling = alpha / r\n",
    "\n",
    "        # For Linear: out_features x in_features -> we create A: r x in, B: out x r\n",
    "        if isinstance(module, nn.Linear):\n",
    "            in_f = module.in_features\n",
    "            out_f = module.out_features\n",
    "            self.A = nn.Parameter(torch.zeros(r, in_f))\n",
    "            self.B = nn.Parameter(torch.zeros(out_f, r))\n",
    "            nn.init.kaiming_uniform_(self.A, a=5**0.5)\n",
    "            nn.init.zeros_(self.B)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # freeze the base\n",
    "        for p in self.module.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        base = self.module(x)\n",
    "        delta = (x @ self.A.T) @ self.B.T\n",
    "        return base + delta * self.scaling\n",
    "\n",
    "\n",
    "def find_parent_and_attr(model, module_name: str):\n",
    "    parts = module_name.split(\".\")\n",
    "    parent = model\n",
    "    for p in parts[:-1]:\n",
    "        # handle list indices in module names like blocks.3.attn\n",
    "        if p.isdigit():\n",
    "            parent = parent[int(p)]\n",
    "        else:\n",
    "            parent = getattr(parent, p)\n",
    "    return parent, parts[-1]\n",
    "\n",
    "\n",
    "def apply_lora_to_linear_modules(\n",
    "    model: nn.Module, target_modules: list[str], r=4, alpha=None\n",
    "):\n",
    "    for name, mod in model.named_modules():\n",
    "        if isinstance(mod, nn.Linear) and any([tm in name for tm in target_modules]):\n",
    "            print(name)\n",
    "            parent, attr = find_parent_and_attr(model, name)\n",
    "            wrapped = LoRAAdapter(mod, r=r, alpha=alpha)\n",
    "            wrapped.to(mod.weight.device)\n",
    "            setattr(parent, attr, wrapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac6335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Total params: 122,976,001; Trainable params: 122,976,001 (100.0000%)\n",
      "Trainable parameter groups (name, numel):\n",
      "  layer1.weight 64000\n",
      "  layer1.bias 6400\n",
      "  layer2.weight 40960000\n",
      "  layer2.bias 6400\n",
      "  layer3.weight 40960000\n",
      "  layer3.bias 6400\n",
      "  layer4.weight 40960000\n",
      "  layer4.bias 6400\n",
      "  output.weight 6400\n",
      "  output.bias 1\n",
      "layer2\n",
      "layer3\n",
      "layer4\n",
      "Total params: 123,283,201; Trainable params: 384,001 (0.3115%)\n",
      "Trainable parameter groups (name, numel):\n",
      "  layer1.weight 64000\n",
      "  layer1.bias 6400\n",
      "  layer2.A 51200\n",
      "  layer2.B 51200\n",
      "  layer3.A 51200\n",
      "  layer3.B 51200\n",
      "  layer4.A 51200\n",
      "  layer4.B 51200\n",
      "  output.weight 6400\n",
      "  output.bias 1\n"
     ]
    }
   ],
   "source": [
    "class BigMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(BigMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = torch.relu(self.layer3(x))\n",
    "        x = torch.relu(self.layer4(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "input_dim=10\n",
    "hidden_dim = 6400\n",
    "output_dim = 1\n",
    "model = BigMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "print_trainable_summary(model)\n",
    "\n",
    "apply_lora_to_linear_modules(\n",
    "    model, target_modules=[\"layer2\", \"layer3\", \"layer4\"], r=8, alpha=32\n",
    ")\n",
    "print_trainable_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de502dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/lora_finetune/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/lpozzi/Git/data-science-lectures/lora_finetune/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 296.7354, Time: 2.52s, GPU Memory: 490.94 MB, GPU Utilization: 2%\n",
      "Epoch [2/10], Loss: 300.0705, Time: 2.76s, GPU Memory: 490.94 MB, GPU Utilization: 46%\n",
      "Epoch [3/10], Loss: 298.0779, Time: 2.98s, GPU Memory: 490.94 MB, GPU Utilization: 62%\n",
      "Epoch [4/10], Loss: 303.6243, Time: 3.19s, GPU Memory: 490.94 MB, GPU Utilization: 70%\n",
      "Epoch [5/10], Loss: 300.2487, Time: 3.41s, GPU Memory: 490.94 MB, GPU Utilization: 71%\n",
      "Epoch [6/10], Loss: 299.3167, Time: 3.63s, GPU Memory: 490.94 MB, GPU Utilization: 68%\n",
      "Epoch [7/10], Loss: 297.3388, Time: 3.84s, GPU Memory: 490.94 MB, GPU Utilization: 71%\n",
      "Epoch [8/10], Loss: 294.4271, Time: 4.07s, GPU Memory: 490.94 MB, GPU Utilization: 71%\n",
      "Epoch [9/10], Loss: 300.3594, Time: 4.28s, GPU Memory: 490.94 MB, GPU Utilization: 68%\n",
      "Epoch [10/10], Loss: 306.8770, Time: 4.50s, GPU Memory: 490.94 MB, GPU Utilization: 71%\n"
     ]
    }
   ],
   "source": [
    "def get_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "        gpu_utilization = torch.cuda.utilization()  # percentage\n",
    "        return gpu_memory, gpu_utilization\n",
    "    return 0, 0\n",
    "\n",
    "num_samples=1000\n",
    "X = torch.randn(num_samples, input_dim)\n",
    "y = torch.sum(X, dim=1) + torch.randn(num_samples) * 0.1  # sum of inputs with noise\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "total_loss = 0\n",
    "epochs=10\n",
    "\n",
    "# Convert dataset to DataLoader for batch processing\n",
    "dataset = TensorDataset(X, y)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for inputs, labels in data_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    total_loss += epoch_loss\n",
    "    elapsed_time = time.time() - start_time\n",
    "    gpu_memory, gpu_utilization = get_gpu_utilization()\n",
    "\n",
    "    # Print performance metrics every epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, \"\n",
    "            f\"Time: {elapsed_time:.2f}s, \"\n",
    "            f\"GPU Memory: {gpu_memory:.2f} MB, \"\n",
    "            f\"GPU Utilization: {gpu_utilization}%\")\n",
    "\n",
    "total_time = time.time() - start_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lora-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
