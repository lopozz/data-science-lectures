## LLMs in Production: Serving Architectures for Transformers [2h]
Last update: 2025-11-24


1. [API Design and Architecture - Backend Engineering Intro (1 Hour)](https://www.youtube.com/watch?v=XvFmUE-36Kc&t=210s)
2. [LLM Visualization](https://bbycroft.net/llm)
3. [LLM Inference Series: 2. The two-phase process behind LLMsâ€™ responses](https://medium.com/@plienhar/llm-inference-series-2-the-two-phase-process-behind-llms-responses-1ff1ff021cd5)
4. [Inside vLLM: Anatomy of a High-Throughput LLM Inference System](https://blog.vllm.ai/2025/09/05/anatomy-of-vllm.html)
5. [Orca: A Distributed Serving System for Transformer-Based Generative Models](https://www.usenix.org/system/files/osdi22-yu.pdf)
6. [LLM-3: Continuous Batching, Paged Attention, Prefill-Decode Disaggregation](https://hao-ai-lab.github.io/cse234-w25/assets/scribe_notes/march11_scribe.pdf)
7. [LLM Inference: Continuous Batching and PagedAttention](https://insujang.github.io/2024-01-07/llm-inference-continuous-batching-and-pagedattention/#the-top)
8. [OSDI '22 - Orca: A Distributed Serving System for Transformer-Based Generative Models](https://www.youtube.com/watch?v=Ob9PPLxETYU&t=236s)
9. [The KV Cache: Memory Usage in Transformers](https://www.youtube.com/watch?v=80bIUggRJf4)