{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26a8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpozzi/Git/data-science-lectures/vllm_in_prod/.venv/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d9d0c",
   "metadata": {},
   "source": [
    "# KV Cache Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b3a6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(3, 0, 2), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "B = 3\n",
    "\n",
    "Wk = nn.Linear(C, C).float()\n",
    "Wq = nn.Linear(C, C).float()\n",
    "Wv = nn.Linear(C, C).float()\n",
    "\n",
    "cached_K = torch.empty((B,0,C), dtype=torch.int64)\n",
    "cached_V = torch.empty((B,0,C), dtype=torch.int64)\n",
    "print(cached_K)\n",
    "\n",
    "# a = torch.randn(B, 1, C)\n",
    "\n",
    "# print(a.size())\n",
    "# cached_K = torch.cat((cached_K,a), dim=1) # add a tensor to the cache\n",
    "# cached_V = torch.cat((cached_V,a), dim=1)\n",
    "# print(cached_K.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3d0ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 2])\n",
      "torch.Size([3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# simulate the prefill phase, three requests arrive...\n",
    "\n",
    "T = 3\n",
    "X = torch.randn(B, T, C)\n",
    "\n",
    "Q = Wq(X)\n",
    "K = Wk(X)\n",
    "V = Wv(X)\n",
    "\n",
    "# store the K and V in cache\n",
    "cached_K = torch.cat((cached_K,K), dim=1) # add a tensor to the cache\n",
    "cached_V = torch.cat((cached_V,V), dim=1)\n",
    "print(cached_K.size())\n",
    "\n",
    "# calculate the attention score as always...\n",
    "KQ = (\n",
    "    Q @ K.transpose(-2, -1) * C**-0.5\n",
    ")  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "KQ = KQ.masked_fill(\n",
    "    torch.tril(\n",
    "        torch.ones(\n",
    "            T, T, dtype=float\n",
    "        )\n",
    "    )\n",
    "    == 0,\n",
    "    float(\"-inf\"),\n",
    ")  # (B, T, T)\n",
    "KQ = F.softmax(KQ, dim=-1)  # (B, T, T)\n",
    "\n",
    "# perform the weighted aggregation of the values\n",
    "\n",
    "O = KQ @ V  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "print(O.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79db2c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2])\n",
      "torch.Size([3, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we simulate one autoregressive step...\n",
    "\n",
    "x_new = torch.randn(B, 1, C)\n",
    "X_new = torch.cat((X, x_new), dim=1)\n",
    "print(X_new.size())\n",
    "x_new = X_new[:, -1:, :]\n",
    "print(x_new.shape)\n",
    "\n",
    "q_new = Wq(x_new)\n",
    "k_new = Wk(x_new)\n",
    "v_new = Wv(x_new)\n",
    "\n",
    "q_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f257a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "cached_K = torch.cat((cached_K,k_new), dim=1)\n",
    "cached_V = torch.cat((cached_V,v_new), dim=1)\n",
    "\n",
    "print(cached_K.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0982d7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KQ = (\n",
    "    q_new @ cached_K.transpose(-2, -1) * C**-0.5\n",
    ")\n",
    "KQ = F.softmax(KQ, dim=-1)\n",
    "print(KQ.size())\n",
    "\n",
    "O = KQ @ cached_V\n",
    "O.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97919980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iter 0: torch.Size([3, 1, 2])\n",
      "cache size: 2 x torch.Size([3, 5, 2])\n",
      "torch.Size([3, 5, 2])\n",
      "\n",
      "\n",
      "Iter 1: torch.Size([3, 1, 2])\n",
      "cache size: 2 x torch.Size([3, 6, 2])\n",
      "torch.Size([3, 6, 2])\n",
      "\n",
      "\n",
      "Iter 2: torch.Size([3, 1, 2])\n",
      "cache size: 2 x torch.Size([3, 7, 2])\n",
      "torch.Size([3, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "# Keep simulating the decoding phase\n",
    "\n",
    "# cached_K = torch.empty((0,C), dtype=torch.int64)\n",
    "# cached_V = torch.empty((0,C), dtype=torch.int64)\n",
    "\n",
    "n_iters = 3\n",
    "X = torch.randn(B, n_iters, C)\n",
    "\n",
    "for i in range(n_iters):\n",
    "    \n",
    "    x_new = X[:, -1:, :]\n",
    "    print(f\"\\n\\nIter {i}: {x_new.size()}\")\n",
    "\n",
    "    q_new = Wq(x_new)\n",
    "    k_new = Wk(x_new)\n",
    "    v_new = Wv(x_new)\n",
    "\n",
    "    cached_K = torch.cat((cached_K,k_new), dim=1)\n",
    "    cached_V = torch.cat((cached_V,v_new), dim=1)\n",
    "\n",
    "    print(f\"cache size: 2 x {cached_K.size()}\")\n",
    "\n",
    "    kq = (\n",
    "        q_new @ cached_K.transpose(-2, -1) * C**-0.5\n",
    "    ) # [T+i+1, T+i+1]\n",
    "\n",
    "    kq = kq.masked_fill(\n",
    "        torch.tril(\n",
    "            torch.ones(\n",
    "                cached_K.size()[1], cached_K.size()[1], dtype=float\n",
    "            )\n",
    "        )\n",
    "        == 0,\n",
    "        float(\"-inf\"),\n",
    "    )\n",
    "    kq = F.softmax(kq, dim=-1)\n",
    "\n",
    "    out = kq @ cached_V\n",
    "    print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecddeb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class KVCache():\n",
    "    def __init__(self, batch_size, max_tokens, num_heads, head_dim):\n",
    "        self.kv_cache = torch.empty(\n",
    "            2, batch_size, max_tokens, num_heads, head_dim,\n",
    "        )\n",
    "        self.max_tokens = max_tokens\n",
    "        self.cumulative_length = 0\n",
    "\n",
    "    def update(self, k, v):\n",
    "\n",
    "        start = int(self.cumulative_length)\n",
    "        end = start + k.size(-3)\n",
    "        if end > self.max_tokens:\n",
    "            raise ValueError(\"KVCache overflow: increase max_tokens\")\n",
    "        \n",
    "        self.kv_cache[0, :, start:end, :, :] = k\n",
    "        self.kv_cache[1, :, start:end, :, :] = v\n",
    "\n",
    "        self.cumulative_length += k.size(-3)\n",
    "\n",
    "        return self.kv_cache[0, :, :self.cumulative_length, :, :], self.kv_cache[1, :, :self.cumulative_length, :, :]\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Clear the cache (start a new sequence).\"\"\"\n",
    "        self.cumulative_length = 0\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head scaled dot-product for causal attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, attn_pdrop: float = 0.1, use_cache=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\"embed_dim must be divisible by num_heads\")\n",
    "        \n",
    "        self.use_cache = use_cache\n",
    "        self.kv_cache = None\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wk = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wv = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wo = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        if self.use_cache:\n",
    "            if self.kv_cache is None:\n",
    "                # Prefill: full sequence\n",
    "                self.kv_cache = KVCache(batch_size, 2048, self.num_heads, self.head_dim)\n",
    "            else:\n",
    "                # Decoding: only last token\n",
    "                x = x[:, -1:, :]           # (B, 1, C)\n",
    "\n",
    "    \n",
    "        q = self.Wq(x).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        k = self.Wk(x).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        v = self.Wv(x).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        if self.use_cache:\n",
    "            k, v = self.kv_cache.update(k, v)   # (B, T_total, nH, H)\n",
    "\n",
    "\n",
    "        q = q.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "        k = k.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "        v = v.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "\n",
    "        kq = (q @ k.transpose(-2, -1)) / (self.head_dim**0.5)  # (B, nH, T, T)\n",
    "\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=kq.device, dtype=torch.bool), diagonal=1\n",
    "        ) # (T, T)\n",
    "        kq = kq.masked_fill(mask, float(\"-inf\"))\n",
    "        att = F.softmax(kq, dim=-1)\n",
    "\n",
    "        o = att @ v\n",
    "\n",
    "        o = o.permute(0, 2, 1, 3).contiguous()  # (B, T, nH, H)\n",
    "        o = o.view(batch_size, seq_len, embed_dim)  # concat heads\n",
    "        o = self.Wo(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "B = 3\n",
    "C = 8\n",
    "nH = 2\n",
    "T = 5\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "multihead = MultiHeadAttention(C, nH, use_cache=1)  # 2 heads with 4 dimensions\n",
    "multihead(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cba2ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill completed!\n",
      "\n",
      "Decoding token 1...\n",
      "\n",
      "Decoding token 2...\n",
      "\n",
      "Decoding token 3...\n"
     ]
    }
   ],
   "source": [
    "B = 3\n",
    "C = 8\n",
    "nH = 2\n",
    "T = 5\n",
    "M = 2048\n",
    "H = 4\n",
    "\n",
    "multihead = MultiHeadAttention(C, nH, use_cache=1)\n",
    "X = torch.randn(3, 3, C)\n",
    "\n",
    "out = multihead(X)\n",
    "X = torch.cat((X, out[:, -1:, :]), dim=1)\n",
    "print('Prefill completed!')\n",
    "for i in range(3):\n",
    "    print(f'\\nDecoding token {i+1}...')\n",
    "    out = multihead(X)\n",
    "    X = torch.cat((X, out[:, -1:, :]), dim=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e6d9f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cache: 1388.83 ms\n",
      "With cache: 1046.34 ms\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Benchmark\n",
    "# ---------------------------\n",
    "import time\n",
    "\n",
    "B, T, C, nH = 3, 3, 4, 2\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "att_no_cache = MultiHeadAttention(C, nH, use_cache=False)\n",
    "att_cache    = MultiHeadAttention(C, nH, use_cache=True)\n",
    "\n",
    "# Measure\n",
    "def bench(model, x, name):\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    t0 = time.time()\n",
    "    for _ in range(1000):\n",
    "        out = model(x)\n",
    "        x = torch.cat((x, out[:, -1:, :]), dim=1)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    print(f\"{name}: { (time.time() - t0)*1000:.2f} ms\")\n",
    "\n",
    "bench(att_no_cache, x, \"No cache\")\n",
    "bench(att_cache, x, \"With cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a313a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive grow-KV using torch.cat:\n",
      "  time               : 0.4722 s\n",
      "  bytes COPIED       : 65.552 GB  (O(T^2))\n",
      "  bytes WRITTEN      : 0.033 GB\n",
      "\n",
      "Preallocated KV:\n",
      "  time               : 0.0252 s\n",
      "  bytes COPIED       : 0.000 GB\n",
      "  bytes WRITTEN      : 0.033 GB\n",
      "\n",
      "Speedup (prealloc vs cat): 18.73x\n"
     ]
    }
   ],
   "source": [
    "# Many implementation  of KV-cache do not reflect how code is writen in production\n",
    "# https://github.com/rasbt/LLMs-from-scratch/blob/main/ch04/03_kv-cache/gpt_with_kv_cache.py\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def grow_kv_cat(num_steps=2000, hidden_dim=1024, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Naive KV growth using torch.cat.\n",
    "    Every step:\n",
    "       kv = torch.cat([kv, new_token], dim=0)\n",
    "    which forces:\n",
    "       - allocation of a new (seq_len+1, hidden_dim) tensor\n",
    "       - full copy of old kv into it\n",
    "    => O(T^2) memory copying\n",
    "    \"\"\"\n",
    "    kv = torch.zeros((1, hidden_dim), device=device)\n",
    "    byteqs_copied = 0\n",
    "    bytes_written = 0\n",
    "\n",
    "    if device == \"cuda\": torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for step in range(1, num_steps + 1):\n",
    "        # new token row (1, hidden_dim)\n",
    "        new_row = torch.full((1, hidden_dim), float(step), device=device)\n",
    "\n",
    "        # bytes copied = size of old_kv\n",
    "        bytes_copied += kv.numel() * kv.element_size()\n",
    "\n",
    "        kv = torch.cat([kv, new_row], dim=0)\n",
    "\n",
    "        # writing the new token row\n",
    "        bytes_written += new_row.numel() * new_row.element_size()\n",
    "\n",
    "    if device == \"cuda\": torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    return elapsed, bytes_copied, bytes_written\n",
    "\n",
    "\n",
    "def prealloc_kv(num_steps=2000, hidden_dim=1024, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Preallocation:\n",
    "       kv = empty(max_len, hidden_dim)\n",
    "    Just write into kv[step].\n",
    "    => O(T) work, no copies of old KV.\n",
    "    \"\"\"\n",
    "    max_len = num_steps + 1\n",
    "    kv = torch.empty((max_len, hidden_dim), device=device)\n",
    "\n",
    "    bytes_copied = 0\n",
    "    bytes_written = 0\n",
    "\n",
    "    if device == \"cuda\": torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for step in range(1, num_steps + 1):\n",
    "        kv[step] = step\n",
    "        bytes_written += hidden_dim * kv.element_size()\n",
    "\n",
    "    if device == \"cuda\": torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    return elapsed, bytes_copied, bytes_written\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_steps = 4000\n",
    "    hidden_dim = 2048\n",
    "    device = \"cuda\"  # or \"cpu\"\n",
    "\n",
    "    t_cat, bytes_cat_copy, bytes_cat_write = grow_kv_cat(num_steps, hidden_dim, device)\n",
    "    t_pre, bytes_pre_copy, bytes_pre_write = prealloc_kv(num_steps, hidden_dim, device)\n",
    "\n",
    "    print(f\"Naive grow-KV using torch.cat:\")\n",
    "    print(f\"  time               : {t_cat:.4f} s\")\n",
    "    print(f\"  bytes COPIED       : {bytes_cat_copy / 1e9:.3f} GB  (O(T^2))\")\n",
    "    print(f\"  bytes WRITTEN      : {bytes_cat_write / 1e9:.3f} GB\")\n",
    "\n",
    "    print(f\"\\nPreallocated KV:\")\n",
    "    print(f\"  time               : {t_pre:.4f} s\")\n",
    "    print(f\"  bytes COPIED       : {bytes_pre_copy / 1e9:.3f} GB\")\n",
    "    print(f\"  bytes WRITTEN      : {bytes_pre_write / 1e9:.3f} GB\")\n",
    "\n",
    "    print(f\"\\nSpeedup (prealloc vs cat): {t_cat / t_pre:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a771b",
   "metadata": {},
   "source": [
    "# PagedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a99b614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: [], 1: [], 2: []}, {0, 1, 2, 3})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "B = 3\n",
    "C = 4\n",
    "nH = 2\n",
    "H = 2\n",
    "\n",
    "num_blocks = 4\n",
    "block_size = 5\n",
    "\n",
    "kv_cache = torch.zeros((2, num_blocks, block_size, nH, H))  # 2 x (num_blocks, block_size, nH, H)\n",
    "block_table = {i: [] for i in range(B)}\n",
    "free_blocks = set(range(num_blocks))\n",
    "block_table, free_blocks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b281d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate the prefill phase, three requests arrive...\n",
    "\n",
    "T = 3\n",
    "Wk = nn.Linear(C, C).float()\n",
    "Wq = nn.Linear(C, C).float()\n",
    "Wv = nn.Linear(C, C).float()\n",
    "\n",
    "X = torch.randn(B, T, C)\n",
    "\n",
    "K = Wk(X)  # (B,T,C)\n",
    "V = Wv(X)  # (B,T,C)\n",
    "Q = Wq(X)  # (B,T,C)\n",
    "\n",
    "# compute attention as always\n",
    "KQ = (\n",
    "    Q @ K.transpose(-2, -1) * C**-0.5\n",
    ")  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "KQ = KQ.masked_fill(\n",
    "    torch.tril(\n",
    "        torch.ones(\n",
    "            T, T, dtype=float\n",
    "        )\n",
    "    )\n",
    "    == 0,\n",
    "    float(\"-inf\"),\n",
    ")  # (B, T, T)\n",
    "KQ = F.softmax(KQ, dim=-1)  # (B, T, T)\n",
    "V = Wv(X)  # (B,T,C)\n",
    "O = KQ @ V  # (B, T, T) @ (B, T, C) -> (B, T, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "486a2de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1408, -0.9432,  0.3591, -1.0831],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1292, -0.8815,  0.0480, -0.7154],\n",
       "         [ 0.4501, -1.0432, -0.0692, -0.5334],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7015, -1.1552,  0.1455, -0.8109],\n",
       "         [ 0.1893,  0.7872, -0.1970,  0.2375],\n",
       "         [-0.1401, -1.3495,  0.5314, -1.3789]]], grad_fn=<AsStridedBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DISCLAIMER!\n",
    "# For didactive reason we'll set some values of the K matrix to be zero.\n",
    "# This is meant to visualize padding in case of input sequences with \n",
    "# different legths. \n",
    "# In this way it is more intuitive seing how KV blocks are allocated.\n",
    "\n",
    "# choose unique counts of 1s for each request\n",
    "lengths = torch.tensor([1, 2, 5])\n",
    "\n",
    "for i, L in enumerate(lengths):\n",
    "    K[i, L:] = 0  # pad with zeros for vid√¨sualization \n",
    "\n",
    "\n",
    "print(K.size())\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "944ba022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [[0, 1]], 1: [[1, 2]], 2: [[2, 3]]} {3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1408, -0.9432,  0.3591, -1.0831],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1292, -0.8815,  0.0480, -0.7154],\n",
       "         [ 0.4501, -1.0432, -0.0692, -0.5334],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7015, -1.1552,  0.1455, -0.8109],\n",
       "         [ 0.1893,  0.7872, -0.1970,  0.2375],\n",
       "         [-0.1401, -1.3495,  0.5314, -1.3789],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_table = {i: [] for i in range(B)}\n",
    "free_blocks = set(range(num_blocks))\n",
    "req_lens = (K != 0).any(dim=-1).sum(dim=1)\n",
    "\n",
    "\n",
    "# allocate in blocks\n",
    "for b in range(B):\n",
    "    t = 0\n",
    "    while t < req_lens[b].item(): #K.size(1):\n",
    "        # If there's a last block with free space, fill that first\n",
    "        if block_table[b] and block_table[b][-1][1] < block_size:\n",
    "            block_id, filled = block_table[b][-1]\n",
    "        else:\n",
    "            # Need a new block\n",
    "            if not free_blocks:\n",
    "                raise RuntimeError(\"No more free blocks. Implement eviction/preemption here.\")\n",
    "            block_id = free_blocks.pop()\n",
    "            filled = 0\n",
    "            block_table[b].append([block_id, 0])  # store mutable [block_id, filled]\n",
    "\n",
    "        # take = min(block_size - filled, K.size(1) - t)\n",
    "        take = min(block_size - filled, req_lens[b].item() - t)\n",
    "        kv_cache[0, block_id, filled:filled+take, :, :] = K.view(B, T, nH, H)[b, t:t+take, :, :]\n",
    "        kv_cache[1, block_id, filled:filled+take, :, :] = V.view(B, T, nH, H)[b, t:t+take, :, :]\n",
    "\n",
    "        # Update filled count in block_table\n",
    "        block_table[b][-1][1] = filled + take\n",
    "\n",
    "        t += take\n",
    "\n",
    "print(block_table, free_blocks)\n",
    "\n",
    "kv_cache[0].view(4, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "272d20a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_new.size(): torch.Size([3, 4, 4])\n",
      "req_lens: tensor([1, 1, 1])\n",
      "{0: [[0, 2]], 1: [[1, 3]], 2: [[2, 4]]} {3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1408, -0.9432,  0.3591, -1.0831],\n",
       "         [-0.1408, -0.9432,  0.3591, -1.0831],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1292, -0.8815,  0.0480, -0.7154],\n",
       "         [ 0.4501, -1.0432, -0.0692, -0.5334],\n",
       "         [ 0.1292, -0.8815,  0.0480, -0.7154],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7015, -1.1552,  0.1455, -0.8109],\n",
       "         [ 0.1893,  0.7872, -0.1970,  0.2375],\n",
       "         [-0.1401, -1.3495,  0.5314, -1.3789],\n",
       "         [-0.7015, -1.1552,  0.1455, -0.8109],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now one autoregressive step....\n",
    "\n",
    "x_new = torch.randn(B, 1, C)\n",
    "X_new = torch.cat((X, x_new), dim=1)\n",
    "print(f'X_new.size(): {X_new.size()}')\n",
    "x_new = X_new[:, -1:, :]\n",
    "req_lens = (x_new != 0).any(dim=-1).sum(dim=1)\n",
    "print(f'req_lens: {req_lens}')\n",
    "\n",
    "q_new = Wq(x_new)\n",
    "k_new = Wk(x_new)\n",
    "v_new = Wv(x_new)\n",
    "\n",
    "for b in range(B):\n",
    "    t = 0\n",
    "    while t < req_lens[b].item():\n",
    "        # If there's a last block with free space, fill that first\n",
    "        if block_table[b] and block_table[b][-1][1] < block_size:\n",
    "            block_id, filled = block_table[b][-1]\n",
    "        else:\n",
    "            # Need a new block\n",
    "            if not free_blocks:\n",
    "                raise RuntimeError(\"No more free blocks. Implement eviction/preemption here.\")\n",
    "            block_id = free_blocks.pop()\n",
    "            filled = 0\n",
    "            block_table[b].append([block_id, 0])  # store mutable [block_id, filled]\n",
    "\n",
    "        take = min(block_size - filled, req_lens[b].item() - t)\n",
    "        kv_cache[0, block_id, filled:filled+take, :, :] = K.view(B, T, nH, H)[b, t:t+take, :, :]\n",
    "        kv_cache[1, block_id, filled:filled+take, :, :] = V.view(B, T, nH, H)[b, t:t+take, :, :]\n",
    "\n",
    "        # Update filled count in block_table\n",
    "        block_table[b][-1][1] = filled + take\n",
    "\n",
    "        t += take\n",
    "    \n",
    "print(block_table, free_blocks)\n",
    "kv_cache[0].view(4, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "376bc63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1408, -0.9432,  0.3591, -1.0831],\n",
       "         [-0.1408, -0.9432,  0.3591, -1.0831],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1292, -0.8815,  0.0480, -0.7154],\n",
       "         [ 0.4501, -1.0432, -0.0692, -0.5334],\n",
       "         [ 0.1292, -0.8815,  0.0480, -0.7154],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.7015, -1.1552,  0.1455, -0.8109],\n",
       "         [ 0.1893,  0.7872, -0.1970,  0.2375],\n",
       "         [-0.1401, -1.3495,  0.5314, -1.3789],\n",
       "         [-0.7015, -1.1552,  0.1455, -0.8109]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetch dense K/V for each sequence\n",
    "# Find max length across batch (for padding to common T_total)\n",
    "max_len = max([sum(f for _, f in x) for x in block_table.values()])\n",
    "\n",
    "# these two matrices will allow to reuse our attention code\n",
    "# k_full = K.new_zeros(B, nH, max_len, H)\n",
    "# v_full = V.new_zeros(B, nH, max_len, H)\n",
    "k_full = K.new_zeros(B, max_len, nH, H)\n",
    "v_full = V.new_zeros(B, max_len, nH, H)\n",
    "\n",
    "for b in range(B):\n",
    "    cur = 0\n",
    "    for block_id, filled in block_table[b]:\n",
    "        assert filled\n",
    "        \n",
    "        # kv_cache: (2, num_blocks, block_size, nH, H)\n",
    "        k_block = kv_cache[0, block_id, :filled]  # (filled, nH, H)\n",
    "        v_block = kv_cache[1, block_id, :filled]  # (filled, nH, H)\n",
    "\n",
    "        k_full[b,  cur:cur+filled, :, :] = k_block\n",
    "        v_full[b,  cur:cur+filled, :, :] = v_block\n",
    "        cur += filled\n",
    "\n",
    "print(B, max_len, nH*H)\n",
    "k_full.view(B, max_len, nH*H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3df430bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 4]), torch.Size([3, 4, 2, 2]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_new.shape, k_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "193123a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kq = (q_new  @ k_full.view(B, max_len, nH*H).transpose(-2, -1)) / (C**0.5)  # (B, nH, T, T)\n",
    "\n",
    "mask = torch.triu(\n",
    "    torch.ones(1, 1, device=kq.device, dtype=torch.bool), diagonal=1\n",
    ") # (T, T)\n",
    "kq = kq.masked_fill(mask, float(\"-inf\"))\n",
    "att = F.softmax(kq, dim=-1)\n",
    "\n",
    "o = att @ v_full.view(B, max_len, nH*H)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7279435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefill completed!\n",
      "\n",
      "Decoding token 1...\n",
      "\n",
      "Decoding token 2...\n",
      "\n",
      "Decoding token 3...\n"
     ]
    }
   ],
   "source": [
    "class PagedKVCache():\n",
    "    def __init__(self, num_blocks, block_size, num_heads, head_dim):\n",
    "        self.block_size = block_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.block_table = None #{i: [] for i in range(B)}\n",
    "        self.free_blocks = set(range(num_blocks))\n",
    "        self.kv_cache = torch.empty((2, num_blocks, block_size, num_heads, head_dim)) # (2, nB, B, nH, H)\n",
    "\n",
    "    def update(self, k, v):\n",
    "        # allocate in blocks\n",
    "        # batch_size, _, seq_len, _ = k.size()\n",
    "        batch_size, seq_len, _, _ = k.size()\n",
    "        if self.block_table is None:\n",
    "            self.block_table = {i: [] for i in range(batch_size)}\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            t = 0\n",
    "            # while t < k.size(2):\n",
    "            while t < k.size(1):\n",
    "                # If there's a last block with free space, fill that first\n",
    "                if self.block_table[b] and self.block_table[b][-1][1] < self.block_size:\n",
    "                    block_id, filled = self.block_table[b][-1]\n",
    "                else:\n",
    "                    # Need a new block\n",
    "                    if not self.free_blocks:\n",
    "                        raise RuntimeError(\"No more free blocks. Implement eviction/preemption here.\")\n",
    "                    block_id = self.free_blocks.pop()\n",
    "                    filled = 0\n",
    "                    self.block_table[b].append([block_id, 0])  # store mutable [block_id, filled]\n",
    "\n",
    "                # take = min(self.block_size - filled, k.size(2) - t)\n",
    "                take = min(self.block_size - filled, k.size(1) - t)\n",
    "                # print(k.shape)\n",
    "                # print(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "                self.kv_cache[0, block_id, filled:filled+take, :, :] = k.view(batch_size, seq_len, self.num_heads, self.head_dim)[b, t:t+take, :, :]\n",
    "                self.kv_cache[1, block_id, filled:filled+take, :, :] = v.contiguous().view(batch_size, seq_len, self.num_heads, self.head_dim)[b, t:t+take, :, :]\n",
    "                # self.kv_cache[0, block_id, filled:filled+take, :, :] = k.contiguous().view(batch_size, seq_len, self.num_heads, self.head_dim)[b, t:t+take, :, :]\n",
    "                # self.kv_cache[1, block_id, filled:filled+take, :, :] = v.contiguous().view(batch_size, seq_len, self.num_heads, self.head_dim)[b, t:t+take, :, :]\n",
    "\n",
    "                # Update filled count in block_table\n",
    "                self.block_table[b][-1][1] = filled + take\n",
    "\n",
    "                t += take\n",
    "\n",
    "        # Fetch dense K/V for each sequence\n",
    "        # Find max length across batch (for padding to common T_total)\n",
    "        max_len = max([sum(f for _, f in x) for x in self.block_table.values()])\n",
    "\n",
    "        # k_full = k.new_zeros(batch_size, self.num_heads, max_len, self.head_dim)\n",
    "        # v_full = v.new_zeros(batch_size, self.num_heads, max_len, self.head_dim)\n",
    "\n",
    "        k_full = k.new_zeros(batch_size, max_len, self.num_heads, self.head_dim)\n",
    "        v_full = v.new_zeros(batch_size, max_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            cur = 0\n",
    "            for block_id, filled in self.block_table[b]:\n",
    "                assert filled\n",
    "                \n",
    "                # kv_cache: (2, num_blocks, block_size, nH, H)\n",
    "                # slice: (filled, nH, H) -> permute to (nH, filled, H)\n",
    "                k_block = self.kv_cache[0, block_id, :filled]  # (filled, nH, H)\n",
    "                v_block = self.kv_cache[1, block_id, :filled]  # (filled, nH, H)\n",
    "\n",
    "                # k_full[b, :, cur:cur+filled, :] = k_block.permute(1, 0, 2)\n",
    "                # v_full[b, :, cur:cur+filled, :] = v_block.permute(1, 0, 2)\n",
    "\n",
    "                # print(k_full[b, cur:cur+filled,:, :].shape, k_block.shape)\n",
    "\n",
    "                k_full[b, cur:cur+filled,:, :] = k_block #.permute(1, 0, 2)\n",
    "                v_full[b,  cur:cur+filled, :, :] = v_block#.permute(1, 0, 2)\n",
    "                cur += filled\n",
    "\n",
    "        return k_full, v_full\n",
    "\n",
    "num_blocks, block_size = 5, 10\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head scaled dot-product for causal attention.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim: int, num_heads: int, attn_pdrop: float = 0.1, use_cache=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\"embed_dim must be divisible by num_heads\")\n",
    "        \n",
    "        self.use_cache = use_cache\n",
    "        self.kv_cache = None\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.Wq = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wk = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wv = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wo = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(attn_pdrop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_cache and self.kv_cache is not None:\n",
    "            x = x[:, -1:, :]\n",
    "        \n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        if self.use_cache:\n",
    "            if self.kv_cache is None:\n",
    "                # Prefill: full sequence\n",
    "                self.kv_cache = PagedKVCache(num_blocks, block_size, self.num_heads, self.head_dim)\n",
    "            else:\n",
    "                # Decoding: only last token\n",
    "                x = x[:, -1:, :]           # (B, 1, C)\n",
    "\n",
    "    \n",
    "        q = self.Wq(x).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        k = self.Wk(x).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "        v = self.Wv(x).view(batch_size, -1, self.num_heads, self.head_dim)\n",
    "\n",
    "        if self.use_cache:\n",
    "            k, v = self.kv_cache.update(k, v)   # (B, T_total, nH, H)\n",
    "\n",
    "        # print(k.shape)\n",
    "\n",
    "        q = q.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "        k = k.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "        v = v.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "\n",
    "        # q = self.Wq(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        # k = self.Wk(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        # v = self.Wv(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # q = q.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "        # k = k.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "        # v = v.permute(0, 2, 1, 3)  # (B, nH, T, H)\n",
    "\n",
    "        # if self.use_cache:\n",
    "        #     if self.kv_cache is None:\n",
    "        #         # Prefill: initialize kv cache\n",
    "        #         self.kv_cache = PagedKVCache(num_blocks, block_size, self.num_heads, self.head_dim)\n",
    "        #         k, v = self.kv_cache.update(k, v)  # 2 x (B, nH, T, H)\n",
    "        #         print(self.kv_cache.block_table)\n",
    "        #     else:\n",
    "        #         # Decoding: add last token\n",
    "        #         k, v = self.kv_cache.update(k[:, :, -1:, :], v[:, :, -1:, :])\n",
    "\n",
    "        kq = (q @ k.transpose(-2, -1)) / (self.head_dim**0.5)  # (B, nH, T, T)\n",
    "\n",
    "        mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=kq.device, dtype=torch.bool), diagonal=1\n",
    "        ) # (T, T)\n",
    "        kq = kq.masked_fill(mask, float(\"-inf\"))\n",
    "        att = F.softmax(kq, dim=-1)\n",
    "\n",
    "        att = self.dropout(att)\n",
    "\n",
    "        o = att @ v\n",
    "\n",
    "        o = o.permute(0, 2, 1, 3).contiguous()  # (B, T, nH, H)\n",
    "        o = o.view(batch_size, seq_len, embed_dim)  # concat heads\n",
    "        o = self.Wo(o)\n",
    "        o = self.dropout(o)\n",
    "        return o\n",
    "\n",
    "\n",
    "\n",
    "B = 3\n",
    "C = 4\n",
    "nH = 2\n",
    "T = 5\n",
    "H = 2\n",
    "multihead = MultiHeadAttention(C, nH, use_cache=1)  # 2 heads with 5 dimensions\n",
    "X = torch.randn(B, T, C)\n",
    "\n",
    "out = multihead(X)\n",
    "print('Prefill completed!')\n",
    "for i in range(3):\n",
    "    print(f'\\nDecoding token {i+1}...')\n",
    "    out = multihead(X)\n",
    "    X = torch.cat((X,out), dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm-in-prod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
